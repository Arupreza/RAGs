Main Idea and Purpose

The primary goal of this study is to enhance time series anomaly detection in cloud monitoring systems by applying value-based deep reinforcement learning (DRL). Traditional DRL methods struggle with capturing essential temporal patterns, leading to the proposal of improved algorithms—DQN-D, DRQN, and DTQN—that effectively capture time-related information, improving anomaly detection performance.
Working Principle

The study utilizes deep Q-learning principles, incorporating neural networks to approximate Q-values, and applies a dual-network structure with an experience replay mechanism to stabilize learning. LSTM and Transformer models are integrated to capture sequential and contextual temporal information from time series data, enabling robust detection of anomalies.
Workflow

The study's workflow involves abstracting anomaly detection into a DRL framework where the agent interacts with simulated environments using real labels. The agent observes states represented by sequences of KPIs, chooses anomaly/no-anomaly actions, receives rewards based on action-label accuracy, and learns iteratively by sampling experiences from a modified replay pool.
Methodology

Three main methods were proposed:

    DQN-D: Enhanced DQN using double-layer frame stitching to better capture temporal context.

    DRQN: Combines DQN with LSTM and an improved experience replay pool that serializes data storage and retrieval.

    DTQN: Integrates Transformer encoders into the DQN architecture, focusing on capturing extensive temporal dependencies without positional encoding.

Datasets

The experiments used the Yahoo benchmark datasets, specifically benchmarks A1 (real-world traffic data) and A2 (synthetic data), each containing labeled time series of around 1400-1600 timestamps. This dataset is commonly used to benchmark anomaly detection methods in research contexts.
Key Findings

Experimental results demonstrated significant improvements over traditional DQN methods. The proposed DRQN achieved the highest performance, showing a revised F1 score up to 0.98 on the A1 benchmark. The lightweight DQN-D method also achieved performance close to more complex RNN-based methods, proving efficient in terms of computational speed and simplicity.
Advantages

The proposed methods, particularly DRQN and DQN-D, showed robust improvements in capturing temporal context and accurately detecting anomalies. The approach significantly improved performance metrics (up to 62% compared to traditional DQN) and offered a scalable solution using simpler network structures, thus providing computational efficiency.
Limitations

The Transformer-based DTQN method did not surpass DRQN due to its comparatively weaker local temporal context capture ability, especially relevant in localized anomaly scenarios. Additionally, the relatively small size of the Yahoo dataset might limit the full exploitation of Transformer architectures' capabilities.
Comparison with Related Work

Compared with existing approaches like the EXP method (an LSTM-based anomaly detector) and the widely used Twitter Anomaly Detector (seasonal hybrid ESD), the DRQN method proposed in this study outperformed them, especially in terms of revised F1 scores, highlighting the effectiveness of the improvements in experience replay strategies and sequential data handling.
Conclusion

The study concludes that enhancing DRL techniques with better temporal information capture (through LSTM and Transformer architectures) significantly improves anomaly detection performance in KPI time series. Future research should focus on testing these methods on larger and more realistic datasets to further validate and exploit their capabilities​
