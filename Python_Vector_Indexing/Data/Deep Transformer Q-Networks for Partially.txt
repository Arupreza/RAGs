Main Idea and Purpose

The primary goal of the study is to introduce a novel architecture called Deep Transformer Q-Network (DTQN) for reinforcement learning (RL) tasks with partial observability. The motivation arises from the limitations of recurrent neural networks (RNNs), which are traditionally used to handle partial observability but are often fragile and difficult to train. The DTQN leverages transformers to improve history encoding and solve partially observable tasks more effectively and stably.
Working Principle

DTQN incorporates transformers, specifically transformer decoder layers utilizing multi-headed self-attention, to encode historical observation sequences. The core mechanism involves generating Q-values for each timestep in the agent's observation history. Positional encodings, particularly learned encodings, allow the transformer to handle temporal information effectively. Additionally, intermediate Q-value prediction training enables DTQN to learn robust representations.
Workflow

The workflow includes embedding recent observations, adding learned positional encodings, passing these embeddings through transformer decoder layers, and finally projecting to Q-values corresponding to each action. During training, DTQN leverages all generated Q-values across the observation sequence, promoting efficient and robust learning. For execution, only the last observation's Q-values are used to determine the action.
Methodology

The authors employ an off-policy reinforcement learning method, specifically Q-learning with transformer decoders. They apply intermediate Q-value prediction, training DTQN on Q-values predicted for all timesteps in the observation history. Architectural components such as gated skip connections, positional encodings, and identity map reordering are empirically tested and compared to find the optimal configuration.
Datasets

The study evaluates the proposed DTQN method on several challenging partially observable domains:

    Classic POMDPs: Hallway and HeavenHell.

    Gym-gridverse: Memory and Memory-Four-Rooms gridworld environments.

    Car flag: A 1D navigation task.

    Memory cards: A domain designed to test memory capability similar to a card-matching game.

Key Findings

DTQN significantly outperforms baseline methods including Deep Recurrent Q-Network (DRQN), Deep Q-Network (DQN), and simple attention networks. It demonstrates superior learning speed, stability, and final success rates across all tested domains. Intermediate Q-value prediction notably enhances robustness and effectiveness. Attention visualizations indicate DTQN accurately identifies and attends to critical observations within the history.
Advantages

    DTQN robustly encodes historical data through transformers, surpassing the stability and effectiveness of recurrent architectures.

    It leverages intermediate Q-value prediction, significantly improving learning speed and performance.

    The modular transformer architecture allows visualization and interpretation of learned attention patterns, aiding in model interpretability and analysis.

Limitations

    DTQN has a higher computational complexity compared to simpler architectures like DQN and DRQN, requiring more parameters and computational resources.

    Despite efficient GPU utilization, the increased number of parameters can affect scalability in very large-scale scenarios.

    Training transformer models generally requires careful hyperparameter tuning.

Comparison with Related Work

DTQN shows substantial improvement over recurrent network-based methods like DRQN and attention-augmented recurrent networks (ADRQN, DARQN). Unlike previous transformer-based methods that are specialized for offline RL, DTQN effectively utilizes transformers in an online RL setting with robust training procedures. Comparatively, it provides stronger performance, especially in memory-dependent tasks.
Conclusion

The paper concludes that transformers, particularly when coupled with learned positional encodings and intermediate Q-value prediction, are highly effective for partially observable RL domains. Future directions include extending DTQN's capabilities through larger-scale transformer variants like Big Bird or sparse transformers, further refining training procedures, and exploring additional real-world partially observable environments.
